---
title: "Amazon Sentiment Analysis"
format: 
  html:
    toc: true
    theme: cosmo
    sidebar: true
---

# Overview

## **Inspiration**

This dataset can be used to build and evaluate machine learning models, such as sentiment analysis, product recommendation engines, and trend analysis tools. It is particularly useful for natural language processing (NLP) projects and big data analysis.

# Data Cleaning

This dataset contains a vast collection of Amazon product reviews and metadata across 29 categories, totaling over 75.29 million reviews from the year of 2018. The data was collected by scraping Amazon's review pages and includes detailed information about each product and its associated reviews. **Source Link:** <https://amazon-reviews-2023.github.io/>

### File Information

The dataset is organized into 29 CSV files, each corresponding to a different product category. Each file contains reviews and associated metadata.

-   **Amazon_Fashion.csv**: 883,636 reviews

-   **All_Beauty.csv**: 371,345 reviews

-   **Appliances.csv**: 602,777 reviews

-   **Arts_Crafts_and_Sewing.csv**: 2,875,917 reviews

-   **Automotive.csv**: 7,990,166 reviews

-   **Books.csv**: 51,311,621 reviews

-   **CDs_and_Vinyl.csv**: 4,543,369 reviews

-   **Cell_Phones_and_Accessories.csv**: 10,063,255 reviews

-   **Clothing_Shoes_and_Jewelry.csv**: 32,292,099 reviews

-   **Digital_Music.csv**: 1,584,082 reviews

-   **Electronics.csv**: 20,994,353 reviews

-   **Gift_Cards.csv**: 147,194 reviews

-   **Grocery_and_Gourmet_Food.csv**: 5,074,160 reviews

-   **Home_and_Kitchen.csv**: 21,928,568 reviews

-   **Industrial_and_Scientific.csv**: 1,758,333 reviews

-   **Kindle_Store.csv**: 5,722,988 reviews

-   **Luxury_Beauty.csv**: 574,628 reviews

-   **Magazine_Subscriptions.csv**: 89,689 reviews

-   **Movies_and_TV.csv**: 8,765,568 reviews

-   **Musical_Instruments.csv**: 1,512,530 reviews

-   **Office_Products.csv**: 5,581,313 reviews

-   **Patio_Lawn_and_Garden.csv**: 5,236,058 reviews

-   **Pet_Supplies.csv**: 6,542,483 reviews

-   **Prime_Pantry.csv**: 471,614 reviews

-   **Software.csv**: 459,436 reviews

-   **Sports_and_Outdoors.csv**: 12,980,837 reviews

-   **Tools_and_Home_Improvement.csv**: 9,015,203 reviews

-   **Toys_and_Games.csv**: 8,201,231 reviews

-   **Video_Games.csv**: 2,565,349 reviews

### Column Descriptors

Each CSV file contains the following columns:

-   **asin**: The Amazon Standard Identification Number (ASIN) for the product.

-   **reviewText**: The text of the review written by the customer.

-   **overall**: The overall rating given by the customer (1 to 5 stars).

-   **category**: The category of the product.

-   **summary**: A brief summary of the review.

## Feature Engineering

### K-cores

These data have been reduced to extract the [k-core](https://en.wikipedia.org/wiki/Degeneracy_(graph_theory)), such that each of the remaining users and items have k reviews each. This shrinks the size of the dataset to fit the purpose of this project. We then resample data (max 10,000 per category) to further reduce the data size to make it workable.

```{r, eval=FALSE}
# Warning! Do not run this code block if you already have Amazon_Reviews_Processed.csv file in your directory.

library(data.table) # Used for fast data loading and manipulation.
library(stringr) # Provides useful string functions.

# Define thresholds for review length
length_thresholds <- list(small = 128, medium = 256, large = 512)

# Define function to categorize sentiment
categorize_sentiment <- function(overall) {
  if (overall %in% c(1, 2)) {
    return("negative")
  } else if (overall == 3) {
    return("neutral")
  } else {
    return("positive")
  }
}

# Define function to categorize review length
categorize_length <- function(text) {
  if (is.na(text) || text == "") {
    return(NA)  # Return NA for missing or empty values
  }
  
  length <- nchar(as.character(text))  # Convert text to character before measuring length
  
  if (length <= length_thresholds$small) {
    return("small")
  } else if (length <= length_thresholds$medium) {
    return("medium")
  } else if (length <= length_thresholds$large) {
    return("large")
  } else {
    return("outlier")
  }
}

# List all CSV files in the dataset directory
# We loop through multiple CSV files so that we can process all review data, not just one file.
dataset_path <- "data"  # Change this to your dataset path
files <- list.files(dataset_path, pattern = "*.csv", full.names = TRUE)

# Initialize an empty data.table to store the processed data
final_data <- data.table()

# Loop through each file and process data
for (file in files) {
  cat("Processing:", file, "\n")
  
  # Read CSV file
  df <- fread(file)
  
  # Ensure column names are correct
  # Ensures data integrity by skipping files that lack required information.
  if (!("overall" %in% names(df)) || !("reviewText" %in% names(df))) {
    cat("Skipping file due to missing columns:", file, "\n")
    next
  }

  # Apply sentiment and length categorization
  # Converts numerical ratings into sentiment labels
  df[, sentiment := sapply(overall, categorize_sentiment)]
  df[, length := sapply(reviewText, categorize_length)]

  # Remove outlier reviews (long or empty reviews)
  df <- df[length != "outlier"]

  # Initialize list for balanced sampling
  samples <- list()

  # Loop through each sentiment category
  for (sentiment in c("negative", "neutral", "positive")) {
    sentiment_data <- df[sentiment == sentiment]

    # Loop through each length category
    for (length in c("small", "medium", "large")) {
      length_data <- sentiment_data[length == length]

      # Resample data (max 10,000 per category)
      if (nrow(length_data) > 0) {
        sampled_data <- length_data[sample(.N, min(.N, 10000))]
        samples <- append(samples, list(sampled_data))
      }
    }
  }

  # Combine samples
  category_samples <- rbindlist(samples, fill = TRUE)

  # Print statistics for current category
  cat("Category:", str_remove(basename(file), "_5.csv"), "\n")
  cat("Total Samples:", nrow(category_samples), "\n")
  cat("Length Counts:", table(category_samples$length), "\n")
  cat("Sentiment Counts:", table(category_samples$sentiment), "\n")
  cat("--------------------------------------------------\n")

  # Append to final dataset
  final_data <- rbind(final_data, category_samples, fill = TRUE)
}

# Save final processed dataset
fwrite(final_data, "Amazon_Reviews_Processed.csv")

cat("Data processing complete! Saved to Amazon_Reviews_Processed.csv\n")

```

### Define Length Thresholds for Reviews

-   We **categorize reviews based on length** to analyze how review length correlates with sentiment.
-   **Short reviews** might have different sentiments compared to **longer reviews**.

### Categorize Sentiment

-   Star ratings (`overall`) serve as **a proxy for sentiment**.
-   This function converts **numerical ratings into categorical labels** (`negative`, `neutral`, `positive`).

### Categorize Review Length

-   Reviews are of **varying lengths**, and some could be **too long (outliers)**.
-   **Text length affects sentiment analysis**—short reviews may be more **emotional**, while long ones may be more **descriptive**.

### Balance the Dataset

Some categories might have **too many or too few reviews**, so we:

-   **Resample up to 10,000** reviews per category.

-   Ensure **each sentiment (`negative`, `neutral`, `positive`) is evenly distributed**.

# Data Exploration

## Table Optimization

This code block is designed to **optimize memory usage and improve computational efficiency** when working with a large dataset containing Amazon product reviews. The dataset has over **3.1 million observations**, making memory optimization crucial for smooth data processing and analysis. The code achieves this by converting columns to **appropriate data types**, such as:

-   **`overall` (ratings) → `integer`** to reduce memory compared to storing it as numeric.

-   **`category`, `asin`, `length`, and `sentiment` → `factor`** to store categorical values efficiently instead of character strings.

-   **`reviewText` and `summary` → `character`** since they are text-based and do not need factor encoding. By making these conversions, the dataset requires less memory and ensures that **operations like filtering, grouping, and analysis run faster**.

```{r}
library(data.table)  # Efficient data handling

# Load dataset
data <- fread("Amazon_Reviews_Processed.csv")

# Convert data types for memory optimization
data[, overall := as.integer(overall)]            # Convert ratings to integer
data[, category := as.factor(category)]          # Convert categorical data to factor
data[, reviewText := as.character(reviewText)]   # Convert review text to string
data[, summary := as.character(summary)]         # Convert summary to string
data[, asin := as.factor(asin)]                  # Convert product ID to factor
data[, length := as.factor(length)]              # Convert length category to factor
data[, sentiment := as.factor(sentiment)]        # Convert sentiment to factor

# Check memory usage after conversion
print(object.size(data), units = "MB")  # Print memory usage in MB
str(data)  # Display structure of the dataset

```

## Data Summary

The purpose of this block of code is to **provide a comprehensive summary of the dataset** before performing any further analysis. It helps to **understand the structure, distribution, and composition of the data** by computing key statistics such as numerical summaries, categorical distributions, and data types.

```{r}
# Load dataset
data <- fread("Amazon_Reviews_Processed.csv")

# Display numeric statistics (like Python's describe())
print("📊 Basic Statistics:")
summary(data)

# Display structure of the dataset (similar to .info())
print("🔍 Data Structure:")
str(data)

# Display frequency counts for categorical variables
print("📌 Category Distribution:")
print(table(data$category))

print("📌 Sentiment Distribution:")
print(table(data$sentiment))

print("📌 Length Category Distribution:")
print(table(data$length))
```

### Data Imbalance

The dataset consists of **3,127,185 reviews** across **multiple product categories** such as **Books, Electronics, Beauty, and Fashion**. The **`overall` rating** shows a median value of `5.0`, indicating that the dataset is **skewed towards positive reviews**. This is further supported by the **sentiment distribution**, where **2,874,077 (92%) of the reviews are positive**, compared to **125,878 neutral reviews and 127,230 negative reviews**. This suggests a potential **imbalance in sentiment**, which may need adjustment if used for predictive modeling.

### Review Length Difference

The **category distribution** shows that some categories, like **Books (630,000 reviews)** and **Clothing, Shoes, and Jewelry (270,000 reviews)**, have significantly more reviews than smaller categories such as **Appliances (1,557 reviews)**. Additionally, the **review length distribution** reveals that most reviews are **small (1,748,268)**, while medium-length and large reviews make up a smaller portion. This suggests that the majority of customer feedback is **brief**, which could impact text-based sentiment analysis. Understanding these distributions helps us determine whether **resampling, data balancing, or filtering** is necessary for fair and accurate analysis.

-   If some categories **dominate the dataset**, analysis might be **biased** toward those categories.

-   If a category has **too few reviews**, it may need **oversampling or exclusion** to ensure statistical significance.

-   It helps in making **business and research decisions**, such as identifying which product types receive more customer engagement.

# Building A Classification Model

We would like to see whether a review is likely to result in a high (4-5 stars) or low (1-2 stars) rating using logistic regression based on textual features.

### Setup & Data Ingestion

```{r}
# text mining
library(tidytext)
library(textdata)
library(dplyr)
library(ggplot2)

dat<-read.csv('Amazon_Reviews_Processed.csv')

dat<-dat%>%mutate(ID=1:nrow(dat))
```

### Tokenization & Compute TF-IDF

```{r}
temp<-dat%>%
  # splits each reviewText into lowercase tokens
  unnest_tokens(output = word, input = reviewText)%>%
  
  # keeps only tokens that Bing’s lexicon classifies as positive or negative
  inner_join(get_sentiments('bing'))%>%
  
  # collapses duplicate tokens inside each review and stores how often they appear (n
  count(ID, word)%>%
  
  bind_tf_idf(word, ID, n)

```

tf – term frequency in a review

idf – inverse‑document frequency across all reviews

tf_idf – their product, which emphasises words that are common within a review but rare across reviews.

### Bring in the star rating (overall sentiment label)

```{r}
temp<-temp%>%left_join(dat%>%select(ID,overall))
```

After the join each token row now knows the review’s star rating (overall). This lets you test whether certain words predict higher or lower ratings later.

### How many unique words explain most of the corpus?

```{r}
# words with high frequency
temp%>%group_by(word)%>%
  summarise(counts=sum(n))%>%
  arrange(-counts)%>%
  mutate(Proportion=cumsum(counts)/nrow(temp),
         freq_rank=1:4547)%>%
  ggplot(aes(freq_rank,Proportion))+geom_line()+
  geom_hline(yintercept = 0.8,linetype=2,colour='red')+
  geom_vline(xintercept =60,linetype=2,colour='red')
```

### Display the 60 most frequent sentiment‑bearing word

```{r}
# 60 words with highest frequency would capture more than 80% words counts
temp%>%group_by(word)%>%
  summarise(counts=sum(n))%>%
  mutate(word=reorder(word,counts))%>%arrange(-counts)%>%head(60)%>%
  ggplot(aes(y=word,x=counts))+geom_bar(stat = 'identity')

```

### Feature Selection (Dimension Reduction)

```{r}
# features
words<-temp%>%
  group_by(word)%>%
  summarise(counts=sum(n))%>%
  mutate(word=reorder(word,counts))%>%
  arrange(-counts)%>%
  head(60)

words<-as.character(words$word)
```

We limit the feature space so the model doesn’t explode in dimensionality (60 columns instead of thousands)

### Converting the tidy tokens into a document‑term matrix

```{r}
temp2<-temp%>%filter(word%in%words)

# rows　= reviews, cols　= words, value　= idf
dat_feat<-temp2%>%
  cast_dtm(ID, word, idf)%>%
  as.matrix()

# bring back the star rating
dat_feat<-dat%>%
  select(ID,overall)%>%
  left_join(data.frame(ID=as.numeric(rownames(dat_feat)),dat_feat))

# words that don’t appear in a review → 0
dat_feat[is.na(dat_feat)]<-0

```

### Fit a logistic regression and visualise word effects

```{r}
# logistic regression model

# Binary target: 1 if rating < 4 stars (negativeish), 0 otherwise.
mod<-glm(I(overall<4)~.-ID-overall,data=dat_feat,family=binomial())

summary(mod)

# drop the intercept
coef<-coef(mod)[-1]

data.frame(coef,words=names(coef))%>%
  ggplot(aes(x=coef,y=reorder(words,abs(coef))))+
  geom_bar(stat='identity')+
  labs(x='log(OR) of low rating',y='keywords')
```

## Model Evaluation

```{r}
# model evaluation
# confusion matrix
pred<-ifelse(predict(mod)>0,'low','high')

real<-ifelse(dat_feat$overall<4,'low','high')

caret::confusionMatrix(as.factor(pred),as.factor(real))

library(pROC)
roc1<-roc(real~predict(mod))
plot(roc1,print.auc=TRUE)
auc(roc1)
```

0.83 ⇒ **83 % of the time** it orders the pair correctly. \
Values:\
• \> 0.90 = excellent\
• 0.80–0.90 = good\
• 0.70–0.80 = fair\
• 0.50–0.70 = weak

The result is solid for a first pass, but there’s still head‑room before you hit “excellent.”
